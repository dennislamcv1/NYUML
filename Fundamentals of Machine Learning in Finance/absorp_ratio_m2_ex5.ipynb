{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project: Trading Strategy based on PCA\n",
    "\n",
    "Welcome to your course project. This exercise gives you a hands-on experience to use PCA to:\n",
    "\n",
    "- construct eigen-portfolios\n",
    "- implement a measure of market systemic risk\n",
    "- develop simple trading strategy\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to use PCA to construct eigen-portfolios\n",
    "- Be able to use PCA to calculate a measure of market systemic risk\n",
    "- Be able to implement and analyze performance of portfolio strategy\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(â‰ˆ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](http://pandas.pydata.org/) Python data analysis library\n",
    "- [pandas](http://scikit-learn.org/stable/) scikit-learn - machine learning in Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  scikit-learn: 0.18.2\n",
      "  tensorflow: 1.10.1\n",
      "  pandas: 0.19.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(\"  scikit-learn: %s\" % sklearn.__version__)\n",
    "print(\"  tensorflow: %s\" % tf.__version__)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import sklearn.model_selection\n",
    "    import sklearn.linear_model\n",
    "except:\n",
    "    print(\"Looks like an older version of sklearn package\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"  pandas: %s\"% pd.__version__)\n",
    "except:\n",
    "    print(\"Missing pandas package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"LztgGBBtEeiaYgrsftMrjA\" \n",
    "all_parts=[\"oZXnf\", \"ahjZa\", \"9tUbW\",\"wjLiO\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "# COURSERA_EMAIL = # the email\n",
    "COURSERA_TOKEN=\"LBwjlf6IVSOXfWXM\"\n",
    "COURSERA_EMAIL=\"dennislcv1@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset:  daily prices of  stocks from S&P 500 index  ####\n",
    "For this exercise we will be working with S&P 500 Index stock prices dataset. \n",
    "The following cell computes returns based for a subset of S&P 500 index stocks. It starts with stocks price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset prices shape (3493, 419)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-27</th>\n",
       "      <td>46.1112</td>\n",
       "      <td>78.9443</td>\n",
       "      <td>3.9286</td>\n",
       "      <td>4.5485</td>\n",
       "      <td>13.7898</td>\n",
       "      <td>15.6719</td>\n",
       "      <td>48.0313</td>\n",
       "      <td>10.8844</td>\n",
       "      <td>39.5477</td>\n",
       "      <td>8.1250</td>\n",
       "      <td>32.9375</td>\n",
       "      <td>33.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>45.8585</td>\n",
       "      <td>77.8245</td>\n",
       "      <td>3.6295</td>\n",
       "      <td>4.5485</td>\n",
       "      <td>14.2653</td>\n",
       "      <td>14.3906</td>\n",
       "      <td>47.7500</td>\n",
       "      <td>10.7143</td>\n",
       "      <td>38.5627</td>\n",
       "      <td>7.7188</td>\n",
       "      <td>32.3125</td>\n",
       "      <td>33.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>44.5952</td>\n",
       "      <td>78.0345</td>\n",
       "      <td>3.7054</td>\n",
       "      <td>4.3968</td>\n",
       "      <td>14.5730</td>\n",
       "      <td>13.7656</td>\n",
       "      <td>46.7500</td>\n",
       "      <td>10.6576</td>\n",
       "      <td>37.3807</td>\n",
       "      <td>7.6406</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>47.8377</td>\n",
       "      <td>80.7640</td>\n",
       "      <td>3.5804</td>\n",
       "      <td>4.5333</td>\n",
       "      <td>14.7128</td>\n",
       "      <td>13.9688</td>\n",
       "      <td>49.0000</td>\n",
       "      <td>10.8844</td>\n",
       "      <td>37.9717</td>\n",
       "      <td>7.9219</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>51.5434</td>\n",
       "      <td>83.4934</td>\n",
       "      <td>3.5290</td>\n",
       "      <td>4.5788</td>\n",
       "      <td>14.7968</td>\n",
       "      <td>15.3281</td>\n",
       "      <td>48.1250</td>\n",
       "      <td>10.6576</td>\n",
       "      <td>35.9032</td>\n",
       "      <td>7.9688</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A       AA    AAPL     ABC      ABT     ADBE      ADI  \\\n",
       "2000-01-27  46.1112  78.9443  3.9286  4.5485  13.7898  15.6719  48.0313   \n",
       "2000-01-28  45.8585  77.8245  3.6295  4.5485  14.2653  14.3906  47.7500   \n",
       "2000-01-31  44.5952  78.0345  3.7054  4.3968  14.5730  13.7656  46.7500   \n",
       "2000-02-01  47.8377  80.7640  3.5804  4.5333  14.7128  13.9688  49.0000   \n",
       "2000-02-02  51.5434  83.4934  3.5290  4.5788  14.7968  15.3281  48.1250   \n",
       "\n",
       "                ADM      ADP    ADSK      AEE      AEP  \n",
       "2000-01-27  10.8844  39.5477  8.1250  32.9375  33.5625  \n",
       "2000-01-28  10.7143  38.5627  7.7188  32.3125  33.0000  \n",
       "2000-01-31  10.6576  37.3807  7.6406  32.5625  33.5000  \n",
       "2000-02-01  10.8844  37.9717  7.9219  32.5625  33.6875  \n",
       "2000-02-02  10.6576  35.9032  7.9688  32.5625  33.6250  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# load dataset\n",
    "\n",
    "asset_prices = pd.read_csv('/home/jovyan/work/readonly/spx_holdings_and_spx_closeprice.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0).dropna()\n",
    "n_stocks_show = 12\n",
    "print('Asset prices shape', asset_prices.shape)\n",
    "asset_prices.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate daily log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.005480</td>\n",
       "      <td>-0.014185</td>\n",
       "      <td>-0.076134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034482</td>\n",
       "      <td>-0.081758</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.015628</td>\n",
       "      <td>-0.024907</td>\n",
       "      <td>-0.049994</td>\n",
       "      <td>-0.018975</td>\n",
       "      <td>-0.016760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>-0.027548</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>-0.033352</td>\n",
       "      <td>0.021570</td>\n",
       "      <td>-0.043431</td>\n",
       "      <td>-0.020942</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>-0.030651</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>0.072710</td>\n",
       "      <td>0.034978</td>\n",
       "      <td>-0.033735</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.036816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>0.077464</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>-0.014356</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.097310</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>-0.020837</td>\n",
       "      <td>-0.054475</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>0.016340</td>\n",
       "      <td>-0.031014</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>-0.006617</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.126402</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067217</td>\n",
       "      <td>0.035288</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>0.033457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A        AA      AAPL       ABC       ABT      ADBE  \\\n",
       "2000-01-28 -0.005480 -0.014185 -0.076134  0.000000  0.034482 -0.081758   \n",
       "2000-01-31 -0.027548  0.002698  0.020912 -0.033352  0.021570 -0.043431   \n",
       "2000-02-01  0.072710  0.034978 -0.033735  0.031045  0.009593  0.014761   \n",
       "2000-02-02  0.077464  0.033795 -0.014356  0.010037  0.005709  0.097310   \n",
       "2000-02-03  0.016340 -0.031014  0.045537 -0.006617  0.005670  0.126402   \n",
       "\n",
       "                 ADI       ADM       ADP      ADSK       AEE       AEP  \n",
       "2000-01-28 -0.005857 -0.015628 -0.024907 -0.049994 -0.018975 -0.016760  \n",
       "2000-01-31 -0.020942 -0.005292 -0.030651 -0.010131  0.007737  0.015152  \n",
       "2000-02-01  0.048128  0.021281  0.015810  0.036816  0.000000  0.005597  \n",
       "2000-02-02 -0.017857 -0.020837 -0.054475  0.005920  0.000000 -0.001855  \n",
       "2000-02-03  0.098701  0.000000  0.067217  0.035288  0.011516  0.033457  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns = np.log(asset_prices) - np.log(asset_prices.shift(1))\n",
    "asset_returns = asset_prices.pct_change(periods=1)\n",
    "asset_returns = asset_returns.iloc[1:, :]\n",
    "asset_returns.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_returns(r_df):\n",
    "    \"\"\"\n",
    "    Normalize, i.e. center and divide by standard deviation raw asset returns data\n",
    "\n",
    "    Arguments:\n",
    "    r_df -- a pandas.DataFrame of asset returns\n",
    "\n",
    "    Return:\n",
    "    normed_df -- normalized returns\n",
    "    \"\"\"\n",
    "    mean_r = r_df.mean(axis=0)\n",
    "    sd_r = r_df.std(axis=0)\n",
    "    normed_df = (r_df - mean_r) / sd_r\n",
    "    return normed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.190054</td>\n",
       "      <td>-0.513710</td>\n",
       "      <td>-2.714709</td>\n",
       "      <td>-0.049779</td>\n",
       "      <td>2.182933</td>\n",
       "      <td>-2.684131</td>\n",
       "      <td>-0.212461</td>\n",
       "      <td>-0.766996</td>\n",
       "      <td>-1.540731</td>\n",
       "      <td>-1.803947</td>\n",
       "      <td>-1.372991</td>\n",
       "      <td>-0.994169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>-0.898232</td>\n",
       "      <td>0.096888</td>\n",
       "      <td>0.688156</td>\n",
       "      <td>-1.757230</td>\n",
       "      <td>1.355644</td>\n",
       "      <td>-1.438899</td>\n",
       "      <td>-0.720771</td>\n",
       "      <td>-0.279098</td>\n",
       "      <td>-1.891884</td>\n",
       "      <td>-0.391433</td>\n",
       "      <td>0.547298</td>\n",
       "      <td>0.871919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>2.319164</td>\n",
       "      <td>1.264327</td>\n",
       "      <td>-1.227995</td>\n",
       "      <td>1.539597</td>\n",
       "      <td>0.588289</td>\n",
       "      <td>0.451774</td>\n",
       "      <td>1.606541</td>\n",
       "      <td>0.975244</td>\n",
       "      <td>0.948126</td>\n",
       "      <td>1.272129</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>0.313197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>2.471738</td>\n",
       "      <td>1.221529</td>\n",
       "      <td>-0.548494</td>\n",
       "      <td>0.464060</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>3.133764</td>\n",
       "      <td>-0.616815</td>\n",
       "      <td>-1.012898</td>\n",
       "      <td>-3.348109</td>\n",
       "      <td>0.177340</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>-0.122594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>0.510174</td>\n",
       "      <td>-1.122380</td>\n",
       "      <td>1.551619</td>\n",
       "      <td>-0.388563</td>\n",
       "      <td>0.336944</td>\n",
       "      <td>4.078966</td>\n",
       "      <td>3.310577</td>\n",
       "      <td>-0.029293</td>\n",
       "      <td>4.090396</td>\n",
       "      <td>1.217955</td>\n",
       "      <td>0.818989</td>\n",
       "      <td>1.942390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A        AA      AAPL       ABC       ABT      ADBE  \\\n",
       "2000-01-28 -0.190054 -0.513710 -2.714709 -0.049779  2.182933 -2.684131   \n",
       "2000-01-31 -0.898232  0.096888  0.688156 -1.757230  1.355644 -1.438899   \n",
       "2000-02-01  2.319164  1.264327 -1.227995  1.539597  0.588289  0.451774   \n",
       "2000-02-02  2.471738  1.221529 -0.548494  0.464060  0.339454  3.133764   \n",
       "2000-02-03  0.510174 -1.122380  1.551619 -0.388563  0.336944  4.078966   \n",
       "\n",
       "                 ADI       ADM       ADP      ADSK       AEE       AEP  \n",
       "2000-01-28 -0.212461 -0.766996 -1.540731 -1.803947 -1.372991 -0.994169  \n",
       "2000-01-31 -0.720771 -0.279098 -1.891884 -0.391433  0.547298  0.871919  \n",
       "2000-02-01  1.606541  0.975244  0.948126  1.272129 -0.008894  0.313197  \n",
       "2000-02-02 -0.616815 -1.012898 -3.348109  0.177340 -0.008894 -0.122594  \n",
       "2000-02-03  3.310577 -0.029293  4.090396  1.217955  0.818989  1.942390  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_r = center_returns(asset_returns)\n",
    "normed_r.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute Absorption Ratio(AR). We do so by defining a moving look back window over which we collect returns for computing PCA. We start off from the earliest historical data and march forward moving by step_size, which we also choose arbitrary. For each such window we compute PCA and AR, fixing in advance number of components in the enumerator. Specifically, for we use the following hyper-parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (Implement exponentially-weighted)\n",
    "\n",
    "**Instructions:**\n",
    "Implement exponent_weighting function which returns a sequence of $w_j$ as np.array. See below:\n",
    "\n",
    "Define sequence of $X_j$ where $j \\subset [N, 0]$, an integer taking all values in the interval from 0 to N  $$ X_j =  e^{-\\frac{log(2)}{H} \\times  \\space j}$$\n",
    "where H is half-life which determines the speed of decay, and $log$ is natural log function\n",
    "Then a sequence of exponentially decaying weights $w_j$ is defined as $$ w_j = \\frac{X_j}{ \\sum\\limits_{j=0}^N X_j } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: exponent_weighting# GRADE \n",
    "def exponent_weighting(n_periods, half_life = 252):\n",
    "    \"\"\"\n",
    "    Calculate exponentially smoothed normalized (in probability density function sense) weights\n",
    "\n",
    "    Arguments:\n",
    "    n_periods -- number of periods, an integer, N in the formula above\n",
    "    half_life -- half-life, which determines the speed of decay, h in the formula\n",
    "    \n",
    "    Return:\n",
    "    exp_probs -- exponentially smoothed weights, np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_probs = np.zeros(n_periods) # do your own calculation instead of dummy zero array\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    ### ...\n",
    "    for j in range(n_periods):\n",
    "        exp_probs[j] = np.exp(-np.log(2)*j/half_life)\n",
    "    exp_probs = exp_probs/sum(exp_probs)\n",
    "    return exp_probs\n",
    "    \n",
    "    return exp_probs\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efde16284e0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQSEiIrlSUIiISK4UFCIikqv/D/nRDooFN9H+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efde16d8d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_probs = exponent_weighting(252*1)\n",
    "plt.plot(exp_probs, linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful, please check on the coursera grader page for the status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00549361,  0.00547852,  0.00546347,  0.00544846,  0.0054335 ,\n",
       "        0.00541857,  0.00540369,  0.00538885,  0.00537404,  0.00535928,\n",
       "        0.00534456,  0.00532988,  0.00531524,  0.00530064,  0.00528608,\n",
       "        0.00527156,  0.00525708,  0.00524264,  0.00522824,  0.00521388,\n",
       "        0.00519956,  0.00518528,  0.00517103,  0.00515683,  0.00514267,\n",
       "        0.00512854,  0.00511445,  0.0051004 ,  0.00508639,  0.00507242,\n",
       "        0.00505849,  0.00504459,  0.00503074,  0.00501692,  0.00500314,\n",
       "        0.0049894 ,  0.00497569,  0.00496202,  0.00494839,  0.0049348 ,\n",
       "        0.00492125,  0.00490773,  0.00489425,  0.00488081,  0.0048674 ,\n",
       "        0.00485403,  0.0048407 ,  0.0048274 ,  0.00481414,  0.00480092,\n",
       "        0.00478773,  0.00477458,  0.00476146,  0.00474838,  0.00473534,\n",
       "        0.00472233,  0.00470936,  0.00469643,  0.00468353,  0.00467066,\n",
       "        0.00465783,  0.00464504,  0.00463228,  0.00461956,  0.00460687,\n",
       "        0.00459421,  0.00458159,  0.00456901,  0.00455646,  0.00454394,\n",
       "        0.00453146,  0.00451901,  0.0045066 ,  0.00449422,  0.00448188,\n",
       "        0.00446957,  0.00445729,  0.00444505,  0.00443284,  0.00442066,\n",
       "        0.00440852,  0.00439641,  0.00438433,  0.00437229,  0.00436028,\n",
       "        0.0043483 ,  0.00433636,  0.00432445,  0.00431257,  0.00430072,\n",
       "        0.00428891,  0.00427713,  0.00426538,  0.00425367,  0.00424198,\n",
       "        0.00423033,  0.00421871,  0.00420712,  0.00419557,  0.00418404])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_1=list(exp_probs[:100])\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:1],all_parts,submissions)\n",
    "exp_probs[:100]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absorption_ratio(explained_variance, n_components):\n",
    "    \"\"\"\n",
    "    Calculate absorption ratio via PCA. absorption_ratio() is NOT to be used with Auto-Encoder. \n",
    "    \n",
    "    Arguments:\n",
    "    explained_variance -- 1D np.array of explained variance by each pricincipal component, in descending order\n",
    "    \n",
    "    n_components -- an integer, a number of principal components to compute absorption ratio\n",
    "    \n",
    "    Return:\n",
    "    ar -- absorption ratio\n",
    "    \"\"\"\n",
    "    ar = np.sum(explained_variance[:n_components]) / np.sum(explained_variance)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (Implement Linear Auto-Encoder)\n",
    "\n",
    "LinearAutoEncoder class has two fully connected layers and no activation functions between the layers.\n",
    "\n",
    "**Instructions:**\n",
    "- fill missing code within LinearAutoEncoder class\n",
    "- in init() method of LinearAutoEncoder setup neural network\n",
    "    - **self.codings_layer** is a fully connected layer with **n_codings** neurons and no activation function\n",
    "    - **self.outputs** is a fully connected layer with **n_outputs** neurons and no activation function\n",
    "    - define loss function as Mean Square Error between the outputs and inputs referenced by **self.X** in the code\n",
    "    - use AdamOptimizer to optimize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "class LinearAutoEncoder:\n",
    "    \"\"\"\n",
    "    To perform simple PCA, we set activation_fn=None \n",
    "    i.e., all neurons are linear and the cost function is the Mean-Square Error (MSE)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_codings, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        n_outputs = n_inputs\n",
    "        self.destroy()\n",
    "        reset_graph()\n",
    "    \n",
    "        # the inputs are n_inputs x n_inputs covariance matrices\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, n_inputs, n_inputs])\n",
    "        with tf.name_scope(\"lin_ae\"):\n",
    "            self.codings_layer = None\n",
    "            self.outputs = None\n",
    "            ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "            self.codings_layer = tf.layers.dense(self.X,n_codings)\n",
    "            self.outputs = tf.layers.dense(self.codings_layer, n_outputs)\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.reconstruction_loss = None\n",
    "            self.training_op = None\n",
    "            ### START CODE HERE ### (â‰ˆ 4-5 lines of code)\n",
    "            self.reconstruction_loss = tf.reduce_mean(tf.squared_difference(self.X,self.outputs))\n",
    "            self.training_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.reconstruction_loss)\n",
    "            ### END CODE HERE ###\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def destroy(self):\n",
    "        if hasattr(self, 'sess') and self.sess is not None:\n",
    "            self.sess.close()\n",
    "            self.sess = None\n",
    "\n",
    "    def absorption_ratio(self, test_input):\n",
    "        \"\"\"\n",
    "        Calculate absorption ratio based on already trained model\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return test_input, 0.\n",
    "        \n",
    "        with self.sess.as_default():  # do not close session\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: test_input})\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "\n",
    "        return codings[0, :, :], var_explained\n",
    "    \n",
    "    def next_batch(self, X_train, batch_size):\n",
    "        \"\"\"\n",
    "        X_train - np.array of double of size K x N x N, where N is dimensionality of the covariance matrix\n",
    "        batch_size - an integer, number of training examples to feed through the nwtwork at once\n",
    "        \"\"\"\n",
    "        y_batch = None\n",
    "\n",
    "        selected_idx = np.random.choice(tuple(range(X_train.shape[0])), size=batch_size)\n",
    "        X_batch = X_train[selected_idx, :, :]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def train(self, X_train, X_test, n_epochs=5, batch_size=2, verbose=False):\n",
    "        \"\"\"\n",
    "        train simple auto-encoder network\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param n_epochs: number of epochs to use for training the model\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return X_test, 0.\n",
    "        \n",
    "        n_examples = len(X_train)  # number of training examples\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # as_default context manager does not close the session when you exit the context,\n",
    "        # and you must close the session explicitly.\n",
    "        with self.sess.as_default():\n",
    "            self.init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                n_batches = n_examples // min(n_examples, batch_size)\n",
    "                for _ in range(n_batches):\n",
    "                    X_batch, y_batch = self.next_batch(X_train, batch_size)\n",
    "                    self.sess.run(self.training_op, feed_dict={self.X: X_batch})\n",
    "                \n",
    "                if verbose:\n",
    "                    # last covariance matrix from the training sample\n",
    "                    if X_train.shape[0] == 1:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: X_train})\n",
    "                    else:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: np.array([X_train[-1, :, :]])})\n",
    "                    mse_test = self.reconstruction_loss.eval(feed_dict={self.X: X_test})\n",
    "                    print('Epoch %d. MSE Train %.4f, MSE Test %.4f' % (epoch, mse_train, mse_test))\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            test_input = np.array([X_train[-1, :, :]])\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "            # print('Linear Auto-Encoder: variance explained: %.2f' % var_explained)\n",
    "\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: X_test})\n",
    "            # print('Done training linear auto-encoder')\n",
    "\n",
    "        return codings[0, :, :], var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time index and covariance matrix shape 1000 (504, 418)\n",
      "time index and covariance matrix shape 1060 (504, 418)\n",
      "time index and covariance matrix shape 1120 (504, 418)\n",
      "time index and covariance matrix shape 1180 (504, 418)\n",
      "time index and covariance matrix shape 1240 (504, 418)\n"
     ]
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "ix_offset = 1000\n",
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "step_size = 60\n",
    "num_samples = 5\n",
    "lookback_window = 252 * 2   # in (days)\n",
    "num_assets = len(stock_tickers)\n",
    "cov_matricies = np.zeros((num_samples, num_assets, num_assets)) # hold training data\n",
    "\n",
    "# collect training and test data\n",
    "ik = 0\n",
    "for ix in range(ix_offset, min(ix_offset + num_samples * step_size, len(normed_r)), step_size):\n",
    "    ret_frame = normed_r.iloc[ix_offset - lookback_window:ix_offset, :-1]\n",
    "    print(\"time index and covariance matrix shape\", ix, ret_frame.shape)\n",
    "    cov_matricies[ik, :, :] = ret_frame.cov()\n",
    "    ik += 1\n",
    "\n",
    "# the last covariance matrix determines the absorption ratio\n",
    "lin_ae = LinearAutoEncoder(n_inputs=num_assets, n_codings=200)\n",
    "np.array([cov_matricies[-1, :, :]]).shape\n",
    "lin_codings, test_absorp_ratio = lin_ae.train(cov_matricies[ : int((2/3)*num_samples), :, :],\n",
    "                                                np.array([cov_matricies[-1, :, :]]),\n",
    "                                                n_epochs=10, \n",
    "                                                batch_size=5)\n",
    "lin_codings, in_sample_absorp_ratio = lin_ae.absorption_ratio(np.array([cov_matricies[0, :, :]]))\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful, please check on the coursera grader page for the status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39359208195496026, 0.39359208195496026]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_2=[test_absorp_ratio, in_sample_absorp_ratio]\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)\n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:2],all_parts,submissions)\n",
    "[test_absorp_ratio, in_sample_absorp_ratio]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half-life = 252\n",
      "Lookback window = 504\n",
      "Step size = 1\n",
      "Variance Threshold = 0\n",
      "Number of stocks = 418\n",
      "Number of principal components = 83\n"
     ]
    }
   ],
   "source": [
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "half_life = 252             # in (days)\n",
    "lookback_window = 252 * 2   # in (days)\n",
    "num_assets = len(stock_tickers)\n",
    "step_size = 1          # days : 5 - weekly, 21 - monthly, 63 - quarterly\n",
    "\n",
    "# require of that much variance to be explained. How many components are needed?\n",
    "var_threshold = 0.8     \n",
    "\n",
    "# fix 20% of principal components for absorption ratio calculation. How much variance do they explain?\n",
    "absorb_comp = int((1 / 5) * num_assets)  \n",
    "\n",
    "print('Half-life = %d' % half_life)\n",
    "print('Lookback window = %d' % lookback_window)\n",
    "print('Step size = %d' % step_size)\n",
    "print('Variance Threshold = %d' % var_threshold)\n",
    "print('Number of stocks = %d' % num_assets)\n",
    "print('Number of principal components = %d' % absorb_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indexes date on which to compute PCA\n",
    "days_offset = 4 * 252\n",
    "num_days = 6 * 252 + days_offset\n",
    "pca_ts_index = normed_r.index[list(range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size))]\n",
    "\n",
    "# allocate arrays for storing absorption ratio\n",
    "pca_components = np.array([np.nan]*len(pca_ts_index))\n",
    "absorp_ratio = np.array([np.nan]*len(pca_ts_index))\n",
    "lae_ar = np.array([np.nan]*len(pca_ts_index))  # absorption ratio computed by Auto-Encoder \n",
    "\n",
    "# keep track of covariance matricies as we would need them for training Auto-Encoder\n",
    "buf_size = 5\n",
    "cov_matricies = np.zeros((buf_size, num_assets, num_assets))\n",
    "\n",
    "exp_probs = exponent_weighting(lookback_window, half_life)\n",
    "assert 'SPX' not in normed_r.iloc[:lookback_window, :-1].columns.values, \"By accident included SPX index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "- on each loop iteration: \n",
    "    - fit PCA to **cov_mat**\n",
    "    - use fitted pca model to pass values to absorption_ratio(). The result of absorption ratio calculation goes into **absorp_ratio**\n",
    "    - compute number of principal components it takes to explain at least **var_threshold** of variance. The result of this calculation goes into **pca_components** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging AE 2006-02-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# run the main loop computing PCA and absorption at each step using moving window of returns  \n",
    "# run this loop using both exponentially weighted returns and equally weighted returns\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "ik = 0\n",
    "use_ewm = False\n",
    "lin_ae = None\n",
    "time_start = time.time()\n",
    "for ix in range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size):\n",
    "    ret_frame = normed_r.iloc[ix - lookback_window:ix, :-1]  # fixed window\n",
    "    # ret_frame = normed_r.iloc[:ix, :-1]  # ever-growing window\n",
    "    if use_ewm:\n",
    "        ret_frame = (ret_frame.T * exp_probs).T\n",
    "    \n",
    "    cov_mat = ret_frame.cov()\n",
    "    circ_idx = ik % buf_size\n",
    "    cov_matricies[circ_idx, :, :] = cov_mat.values\n",
    "\n",
    "    if ik == 0 or ik % 21 == 0:\n",
    "        ### START CODE HERE ### (â‰ˆ 4-5 lines of code)\n",
    "        ### fit PCA, compute absorption ratio by calling absorption_ratio()\n",
    "        ### store result into pca_components for grading\n",
    "        pca = PCA().fit(cov_mat)\n",
    "        absorp_ratio[ik]= absorption_ratio(pca.explained_variance_, absorb_comp)\n",
    "        var_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "        pca_components[ik] = np.where(np.logical_not(var_explained < var_threshold))[0][0] + 1\n",
    "        ### END CODE HERE ###\n",
    "    else:\n",
    "        absorp_ratio[ik] = absorp_ratio[ik-1] \n",
    "        pca_components[ik] = pca_components[ik-1]\n",
    "    \n",
    "    if ik == 0 or ik % 252 == 0:        \n",
    "        if lin_ae is not None:\n",
    "            lin_ae.destroy()\n",
    "\n",
    "        print('Trainging AE', normed_r.index[ix])\n",
    "        lin_ae = LinearAutoEncoder(cov_mat.shape[0], absorb_comp)\n",
    "        lin_codings, lae_ar[ik] = lin_ae.train(cov_matricies[:circ_idx + 1, :, :], np.array([cov_mat.values]),batch_size=2)\n",
    "    else:\n",
    "        lin_codings, lae_ar[ik] = lin_ae.absorption_ratio(np.array([cov_mat.values]))\n",
    "\n",
    "    ik += 1\n",
    "    \n",
    "print ('Absorption Ratio done! Time elapsed: {} seconds'.format(time.time() - time_start))    \n",
    "ts_pca_components = pd.Series(pca_components, index=pca_ts_index)\n",
    "ts_absorb_ratio = pd.Series(absorp_ratio, index=pca_ts_index)\n",
    "ts_lae_absorb_ratio = pd.Series(lae_ar, index=pca_ts_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_absorb_ratio.plot(figsize=(12,6), title='Absorption Ratio via PCA', linewidth=3)\n",
    "plt.savefig(\"Absorption_Ratio_SPX.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_lae_absorb_ratio.plot(figsize=(12,6), title='Absorption Ratio via Auto-Encoder', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed daily (this means the step size is 1) Absorption Ratio times series, we further follow M. Kritzman to make use of AR to define yet another measure: AR Delta. In particular:\n",
    "$$ AR\\delta = \\frac{AR_{15d} - AR_{1y}}{ AR\\sigma_{1y}}$$\n",
    "We use  $AR\\delta$ to build simple portfolio trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following Kritzman and computing AR_delta = (15d_AR -1yr_AR) / sigma_AR\n",
    "ts_ar = ts_absorb_ratio\n",
    "ar_mean_1yr = ts_ar.rolling(252).mean()\n",
    "ar_mean_15d = ts_ar.rolling(15).mean()\n",
    "ar_sd_1yr = ts_ar.rolling(252).std()\n",
    "ar_delta = (ar_mean_15d - ar_mean_1yr) / ar_sd_1yr    # standardized shift in absorption ratio\n",
    "\n",
    "df_plot = pd.DataFrame({'AR_delta': ar_delta.values, 'AR_1yr': ar_mean_1yr.values, 'AR_15d': ar_mean_15d.values}, \n",
    "                       index=ts_ar.index)\n",
    "df_plot = df_plot.dropna()\n",
    "if df_plot.shape[0] > 0:\n",
    "    df_plot.plot(figsize=(12, 6), title='Absorption Ratio Delta', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (AR Delta Trading Strategy)\n",
    "\n",
    "**Instructions:** Implement get_weight() function\n",
    "\n",
    "The AR Delta trading strategy forms a portfolio of EQ and FI, following these simple rules:\n",
    "\n",
    "* __$ -1\\sigma < AR < +1\\sigma $__\t 50 / 50 weights for EQ / FI\n",
    "* __$ AR > +1\\sigma $__\t             0 / 100 weights for EQ / FI\n",
    "* __$ AR < -1\\sigma $__\t             100 / 0 weights for EQ / FI\n",
    "\n",
    "Here we compute AR Delta strategy weights using data from the same data set. As expected, the average number of trades per year is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_weight\n",
    "def get_weight(ar_delta):\n",
    "    '''\n",
    "    Calculate EQ / FI portfolio weights based on Absorption Ratio delta\n",
    "    Arguments:\n",
    "    ar_delta -- Absorption Ratio delta\n",
    "    \n",
    "    Return: \n",
    "        wgts -- a vector of portfolio weights\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 6 lines of code)\n",
    "    ### ....\n",
    "    if ar_delta > 1:\n",
    "        EQ,FI = 0, 1\n",
    "    elif ar_delta < -1:\n",
    "        EQ,FI = 1,0\n",
    "    else:\n",
    "        EQ,FI = 0.5,0.5\n",
    "    wgts = [EQ, FI] # replace with your own calculation here\n",
    "    return wgts\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "ar_delta_data = ar_delta[251:]\n",
    "\n",
    "rebal_dates = np.zeros(len(ar_delta_data))\n",
    "wgts = pd.DataFrame(data=np.zeros((len(ar_delta_data.index), 2)), index=ar_delta_data.index, columns=('EQ', 'FI'))\n",
    "\n",
    "prtf_wgts = get_weight(ar_delta_data.values[0])\n",
    "wgts.iloc[0, :] = prtf_wgts\n",
    "for ix in range(1, len(ar_delta_data)):\n",
    "    prtf_wgts = get_weight(ar_delta_data.values[ix])\n",
    "    wgts.iloc[ix, :] = prtf_wgts\n",
    "    if wgts.iloc[ix-1, :][0] != prtf_wgts[0]:\n",
    "        prtf_wgts = wgts.iloc[ix, :]\n",
    "        rebal_dates[ix] = 1\n",
    "\n",
    "ts_rebal_dates = pd.Series(rebal_dates, index=ar_delta_data.index)\n",
    "ts_trades_per_year = ts_rebal_dates.groupby([ts_rebal_dates.index.year]).sum()\n",
    "print('Average number of trades per year %.2f' % ts_trades_per_year.mean())\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "wgts_test = wgts.as_matrix()\n",
    "idx_row = np.random.randint(low=0, high=wgts_test.shape[0], size=100)\n",
    "np.random.seed(42)\n",
    "idx_col = np.random.randint(low=0, high=wgts_test.shape[1], size=100)\n",
    "\n",
    "# grading\n",
    "part_3=list(wgts_test[idx_row, idx_col])\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:3],all_parts,submissions)\n",
    "\n",
    "wgts_test[idx_row, idx_col]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that weights have been determined, run the re-balancing strategy using time series of returns and compute\n",
    " - sharpe of the strategy\n",
    " - strategy annualized return\n",
    " - strategy annualized volatility\n",
    "\n",
    "Contrast this with 50 / 50 Equity / Fixed Income ETF strategy performance using the same performance metrics. Use VTI as Equity and AGG as Fixed Income assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etf_r= pd.read_csv('/home/jovyan/work/readonly/pca_hw5_etf_returns.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0)\n",
    "etf_prices = pd.read_csv('/home/jovyan/work/readonly/millenials_portfolio_etfs.csv',\n",
    "                         date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                         index_col = 0)\n",
    "etf_returns = etf_prices.pct_change(periods=1)\n",
    "etf_returns = etf_returns.iloc[1450:, :]\n",
    "etf_r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (Calculate performance of backtested strategy)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Implement function backtest_strategy which given a DataFrame of strategy weights and a DataFrame asset returns annualized return, volatility and Sharpe ratio of a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backtest_strategy\n",
    "def backtest_strategy(strat_wgts, asset_returns, periods_per_year = 252):\n",
    "    '''\n",
    "    Calculate portfolio returns and return portfolio strategy performance\n",
    "    Arguments:\n",
    "    \n",
    "    strat_wgts -- pandas.DataFrame of weights of the assets\n",
    "    asset_returns -- pandas.DataFrame of asset returns\n",
    "    periods_per_year -- number of return observations per year\n",
    "    \n",
    "    Return: \n",
    "        (ann_ret, ann_vol, sharpe) -- a tuple of (annualized return, annualized volatility, sharpe ratio)\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 10 lines of code)\n",
    "    backtest = strat_wgts['EQ']*asset_returns['VTI'] + strat_wgts['FI']*asset_returns['AGG']\n",
    "    backtest.dropna(inplace = True)\n",
    "    n = len(backtest)\n",
    "    ann_ret = np.prod(1+ backtest)**(periods_per_year/n)-1\n",
    "    ann_vol = backtest.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_ret/ann_vol\n",
    "    return (ann_ret, ann_vol, sharpe)\n",
    "    # return 0., 0., 1. # annualized return,  annualized volatility,  sharp ratio\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "ann_ret, ann_vol, sharpe = backtest_strategy(wgts, etf_r)\n",
    "print('Absorption Ratio strategy:', ann_ret, ann_vol, sharpe)\n",
    "\n",
    "eq_wgts = wgts.copy()\n",
    "eq_wgts.iloc[:, ] = 0.5\n",
    "ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt = backtest_strategy(eq_wgts, etf_r)\n",
    "print('Equally weighted:', ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt)\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_4=[ann_ret, ann_vol, sharpe, ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt]\n",
    "try:\n",
    "    part4 = \" \".join(map(repr, part_4))\n",
    "except TypeError:\n",
    "    part3 = repr(part_4)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:4],all_parts,submissions)\n",
    "[ann_ret, ann_vol, sharpe, ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "machine-learning-in-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
